{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8bae2a9",
   "metadata": {},
   "source": [
    "# Create Topic\n",
    "- Condition: running kafka docker compose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c45d22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error creating topic comment_events: KafkaError{code=TOPIC_ALREADY_EXISTS,val=36,str=\"Topic 'comment_events' already exists.\"}\n"
     ]
    }
   ],
   "source": [
    "# create_topic.py\n",
    "\n",
    "from confluent_kafka.admin import AdminClient, NewTopic\n",
    "\n",
    "conf = {\"bootstrap.servers\": \"localhost:9094\"}\n",
    "admin_client = AdminClient(conf)\n",
    "\n",
    "topic_name = \"test_topic\"\n",
    "\n",
    "# Create topic with 1 partition and replication factor 1\n",
    "new_topic = NewTopic(topic_name, num_partitions=1, replication_factor=1)\n",
    "\n",
    "# Create topic\n",
    "fs = admin_client.create_topics([new_topic])\n",
    "\n",
    "for topic, f in fs.items():\n",
    "    try:\n",
    "        f.result()  # Wait for the result\n",
    "        print(f\"Topic {topic} created successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating topic {topic}: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078ee267",
   "metadata": {},
   "source": [
    "# The Producer\n",
    "- Stimulate sending events to kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e938d9ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starts producing messages...\n",
      "Record b'0' successfully produced to test_topic [0] at offset 5\n",
      "Record b'1' successfully produced to test_topic [0] at offset 6\n",
      "Record b'2' successfully produced to test_topic [0] at offset 7\n",
      "Record b'3' successfully produced to test_topic [0] at offset 8\n",
      "Record b'4' successfully produced to test_topic [0] at offset 9\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# producer.py\n",
    "\n",
    "from confluent_kafka import Producer\n",
    "import json\n",
    "import time\n",
    "\n",
    "# 1. Configuration\n",
    "conf = {\"bootstrap.servers\": \"localhost:9094\",\n",
    "        \"client.id\": \"producer-1\"}\n",
    "\n",
    "# 2. Create Producer instance\n",
    "producer = Producer(conf)\n",
    "\n",
    "# 3. Callback for delivery report\n",
    "def delivery_report(err, msg):\n",
    "    if err is not None:\n",
    "        print(f\"Delivery failed for record {msg.key()}: {err}\")\n",
    "    else:\n",
    "        print(f\"Record {msg.key()} successfully produced to {msg.topic()} [{msg.partition()}] at offset {msg.offset()}\")\n",
    "\n",
    "# 4. Produce messages\n",
    "print(\"Starts producing messages...\")\n",
    "for i in range(5):\n",
    "    data = {'user_id': i, 'action': 'login', 'timestamp': time.time()}\n",
    "\n",
    "    # Trigger any availabel delivery report callbacks from previous produce() calls\n",
    "    producer.poll(0)\n",
    "\n",
    "    # Asynchronous produce a message. The delivery report callback will be triggerd from poll()\n",
    "    producer.produce(\n",
    "        topic=topic,\n",
    "        key=str(i), # Key is important for ordering within partitions\n",
    "        value=json.dumps(data).encode('utf-8'),\n",
    "        on_delivery=delivery_report\n",
    "    )\n",
    "    time.sleep(1)\n",
    "\n",
    "# 5. wait for any outstanding messages to be delivered and delivery report callbacks to be triggered\n",
    "producer.flush()\n",
    "print(\"Done\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907e40ab",
   "metadata": {},
   "source": [
    "# The Consumer\n",
    "- Reading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e6fc5010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for messages...\n",
      "Received message: key=0, value={\"user_id\": 0, \"action\": \"login\", \"timestamp\": 1765983919.715296}\n",
      "Received message: key=1, value={\"user_id\": 1, \"action\": \"login\", \"timestamp\": 1765983920.7204201}\n",
      "Received message: key=2, value={\"user_id\": 2, \"action\": \"login\", \"timestamp\": 1765983921.725794}\n",
      "Received message: key=3, value={\"user_id\": 3, \"action\": \"login\", \"timestamp\": 1765983922.730648}\n",
      "Received message: key=4, value={\"user_id\": 4, \"action\": \"login\", \"timestamp\": 1765983923.732661}\n"
     ]
    }
   ],
   "source": [
    "# consumer.py\n",
    "\n",
    "from confluent_kafka import Consumer, KafkaError\n",
    "\n",
    "# 1. Configuration\n",
    "conf = {\"bootstrap.servers\": \"localhost:9094\",\n",
    "        \"group.id\": \"mygroup\",\n",
    "        \"auto.offset.reset\": \"earliest\"} # Start reading from the beginning if no offset is stored\n",
    "\n",
    "# 2. Create Consumer instance\n",
    "consumer = Consumer(conf)\n",
    "\n",
    "# 3. Subscribe to the topic\n",
    "topic = \"test_topic\"\n",
    "consumer.subscribe([topic])\n",
    "\n",
    "# 4. Poll for new messages\n",
    "print(\"Waiting for messages...\")\n",
    "try:\n",
    "    while True:\n",
    "        msg = consumer.poll(timeout=1.0)\n",
    "\n",
    "        if msg is None:\n",
    "            continue\n",
    "        if msg.error():\n",
    "            if msg.error().code() == KafkaError._PARTITION_EOF:\n",
    "                # End of partition event\n",
    "                continue\n",
    "            else:\n",
    "                print(msg.error())\n",
    "                break\n",
    "        print(f\"Received message: key={msg.key().decode('utf-8')}, value={msg.value().decode('utf-8')}\")\n",
    "except KeyboardInterrupt:\n",
    "    pass\n",
    "finally:\n",
    "    consumer.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da14e8f",
   "metadata": {},
   "source": [
    "# Schema Registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "67b5417f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from confluent_kafka import SerializingProducer, DeserializingConsumer\n",
    "from confluent_kafka.schema_registry import SchemaRegistryClient\n",
    "from confluent_kafka.schema_registry.avro import AvroSerializer, AvroDeserializer\n",
    "from confluent_kafka.serialization import StringSerializer, StringDeserializer\n",
    "import time\n",
    "\n",
    "# Configuration\n",
    "KAFKA_BOOTSTRAP = \"localhost:9094\"\n",
    "SCHEMA_REGISTRY_URL = \"http://localhost:8081\"\n",
    "TOPIC_NAME = \"comment_events\"\n",
    "\n",
    "# 1. Define the schema\n",
    "value_schema_str = \"\"\"\n",
    "{\n",
    "    \"doc\": \"Schema for user comments used in livestream assistant agent\",\n",
    "    \"name\": \"CommentEvent\",\n",
    "    \"namespace\": \"com.comment.events\",\n",
    "    \"type\": \"record\",\n",
    "    \"fields\": [\n",
    "    {\n",
    "        \"name\": \"reviewerID\",\n",
    "        \"type\": \"string\",\n",
    "        \"doc\": \"Unique ID of the reviewer\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"reviewText\",\n",
    "        \"type\": \"string\",\n",
    "        \"doc\": \"The full text content of the review\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"comment_time\",\n",
    "        \"type\": \"long\",\n",
    "        \"doc\": \"The time user comment\"\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "def run_test():\n",
    "    # 2. Set up the schema registry client\n",
    "    schema_registry_client = SchemaRegistryClient({\"url\": SCHEMA_REGISTRY_URL})\n",
    "\n",
    "    # 3. Producer setup\n",
    "    avro_serializer = AvroSerializer(schema_registry_client, value_schema_str)\n",
    "    producer_conf = {\n",
    "        \"bootstrap.servers\": KAFKA_BOOTSTRAP,\n",
    "        \"key.serializer\": StringSerializer(\"utf_8\"),\n",
    "        \"value.serializer\": avro_serializer,\n",
    "    }\n",
    "    producer = SerializingProducer(producer_conf)\n",
    "\n",
    "    # 4. Consumer setup\n",
    "    avro_deserializer = AvroDeserializer(schema_registry_client, value_schema_str)\n",
    "    consumer_conf = {\n",
    "        \"bootstrap.servers\": KAFKA_BOOTSTRAP,\n",
    "        \"key.deserializer\": StringDeserializer(\"utf_8\"),\n",
    "        \"value.deserializer\": avro_deserializer,\n",
    "        \"group.id\": f\"group\",\n",
    "        \"auto.offset.reset\": \"earliest\",\n",
    "    }\n",
    "    consumer = DeserializingConsumer(consumer_conf)\n",
    "    consumer.subscribe([TOPIC_NAME])\n",
    "\n",
    "    # 5. Test loop\n",
    "    print(f\"--- Sending test message to {TOPIC_NAME} ---\")\n",
    "    test_message = {\n",
    "        \"reviewerID\": \"user121233\",\n",
    "        \"reviewText\": \"This is a test comment\",\n",
    "        \"comment_time\": int(time.time())\n",
    "    }\n",
    "    producer.produce(\n",
    "        topic=TOPIC_NAME,\n",
    "        key=\"test_key\",\n",
    "        value=test_message,\n",
    "    )\n",
    "    producer.flush()\n",
    "\n",
    "    print(\"Waiting 3 seconds for topic metadata to propagate...\")\n",
    "    time.sleep(5)\n",
    "\n",
    "    # 6. Consume the message\n",
    "    print(\"--- Waiting for message recovery ---\")\n",
    "    count = 0\n",
    "    while count < 10: # Increased retries\n",
    "        try:\n",
    "            msg = consumer.poll(timeout=5.0)\n",
    "            if msg is None:\n",
    "                print(\"No message yet, polling...\")\n",
    "                count += 1\n",
    "                continue\n",
    "            \n",
    "            if msg.error():\n",
    "                # This catches the UNKNOWN_TOPIC error without crashing\n",
    "                print(f\"Wait... {msg.error()}\")\n",
    "                time.sleep(1)\n",
    "                count += 1\n",
    "                continue\n",
    "\n",
    "            print(f\"SUCCESS: Recovered data from Kafka: {msg.value()}\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"Polling error: {e}\")\n",
    "            time.sleep(1)\n",
    "            count += 1\n",
    "    consumer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "581ed7b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Sending test message to comment_events ---\n",
      "Waiting 3 seconds for topic metadata to propagate...\n",
      "--- Waiting for message recovery ---\n",
      "SUCCESS: Recovered data from Kafka: {'reviewerID': 'user121233', 'reviewText': 'This is a test comment', 'comment_time': 1766049682}\n"
     ]
    }
   ],
   "source": [
    "run_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1e2c5f",
   "metadata": {},
   "source": [
    "#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
